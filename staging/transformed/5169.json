{"location": {"country": "US", "locality": "United States", "region": "United States", "postal_code": "99999", "street_address": null, "latitude": 39.50355, "longitude": -99.0183}, "salary": {"currency": null, "min_value": null, "max_value": null, "unit": null}, "job": {"title": "Senior Data Engineer (Python, AWS)", "industry": "Information Technology and Services", "description": "About Us\nDescription\nWavicle Data Solutions\ndesigns and delivers data and analytics solutions to reduce time, cost, and risk of companies\u2019 data projects, improving the quality of their analytics and decisions now and into the future\n.\nAs a privately-held consulting service organization with popular, name brand clients across multiple industries, Wavicle offers exciting opportunities for data scientists, solutions architects, developers, and consultants to jump right in and contribute to meaningful, innovative solutions.\nOur 250+ local, nearshore and offshore consultants, data architects, cloud engineers, and developers build cost-effective, right-fit solutions leveraging our team\u2019s deep business acumen and knowledge of cutting-edge data and analytics technology and frameworks.\nWavicle Has Been Recognized By Industry Leaders As Follows\nAt Wavicle, you\u2019ll find a challenging and rewarding work environment where we enjoy working as a team to exceed client expectations. Employees appreciate being part of something meaningful at Wavicle.\nChicago Tribune\u2019s Top Workplaces\nInc 500 Fastest Growing Private Companies in the US\nCrain\u2019s Fast 50 fastest growing companies in the Chicago area\nTalend Expert Partner recognition\nMicrosoft Gold Data Platform competency\nAbout The Role\nWe are looking for a\nSenior Data Engineer\nwho will be responsible for designing and building optimized data pipelines, in an on-prem or cloud environment, for the purpose of driving analytic insights.\nResponsibilities\nCreate the conceptual, logical and physical data models.\nBuild the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of sources like Hadoop, Spark, AWS Lambda, etc.\nLead and/or mentor a small team of data engineers.\nDesign, develop, test, deploy, maintain and improve data integration pipeline.\nDevelop pipeline objects using Apache Spark / Pyspark / Python or Scala.\nDesign and develop data pipeline architectures using Hadoop, Spark and related AWS Services.\nLoad and performance test data pipelines built using the above-mentioned technologies.\nCommunicate effectively with client leadership and business stakeholders.\nParticipate in proposal and/or SOW development.\nRequirements\nProfessional work experience as a strategic or a management consulting (customer facing) role and in an on-shore capacity, is highly preferred.\n5+ years of professional work experience designing and implementing data pipelines in on-prem and cloud environments is REQUIRED.\n5+ years of experience building conceptual, logical and/or physical database designs using tools such as ErWin, Visio or Enterprise Architect.\nStrong hands-on experience implementing big-data solutions in the Hadoop ecosystem (Apache Hadoop, MapReduce, Hive, Pig, Sqoop, NoSQL, etc) and.or Databricks is required.\n3+ years of experience with Cloud platforms (AWS preferred, GCP or Azure), and Python programming and frameworks (e.g., Django, Flask, Bottle) is REQUIRED.\n5+ years of working with one or more databases like Snowflake, AWS Redshift, Oracle, SQL Server, Teradata, Netezza, Hadoop, Mongo DB or Cassandra is required.\nExpert level knowledge of using SQL to write complex, highly-optimized queries across large volumes of data is required.\n3+ years of professional hands-on experience working with one or more ETL tools to build data pipelines/data warehouses is highly preferred (e.g. Talend Big Data, Informatica, DataStage, Abinitio).\n3+ years of hands-on programming experience using Scala, Python, R, or Java is REQUIRED.\n2+ years of professional work experience on ETL pipeline implementation using AWS services such as Glue, Lambda, EMR, Athena, S3, SNS, Kinesis, Data-Pipelines, Pyspark, etc.\n2+ years of professional work experience using real-time streaming systems (Kafka/Kafka Connect, Spark, Flink or AWS Kinesis) is required.\nKnowledge or experience in architectural best practices in building data lakes is required.\nStrong problem solving and troubleshooting skills with the ability to exercise mature judgement.\nAbility to work independently, and provide guidance to junior data engineers.\nAbility to build and maintain strong customer relationships.\nBachelor or Master\u2019s degree in Computer Science, Engineering, Information Systems or relevant degree is required.\nOpen to 25% national travel to the client location, as required by the client.\nEqual Opportunity Employer\nWavicle is an Equal Opportunity Employer and committed to creating an inclusive environment for all employees. We welcome and encourage diversity in the workplace regardless of race, color, religion, national origin, gender, pregnancy, sexual orientation, gender identity, age, physical or mental disability, genetic information or veteran status.\nBenefits\nHealth Care Plan (Medical, Dental & Vision)\nRetirement Plan (401k, IRA)\nLife Insurance (Basic, Voluntary & AD&D)\nUnlimited Paid Time Off (Vacation, Sick & Public Holidays)\nShort Term & Long Term Disability\nTraining & Development\nWork From Home\nCollege Tuition Benefit\nBonus Program", "employment_type": "FULL_TIME", "date_posted": "2021-08-25T17:23:29.000Z"}, "company": {"name": "Wavicle Data Solutions", "link": "https://www.linkedin.com/company/wavicle-data-solutions"}, "education": {"required_credential": "bachelor degree"}, "experience": {"months_of_experience": 60, "seniority_level": "Senior"}}